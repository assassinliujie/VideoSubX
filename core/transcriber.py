import os
from core.utils import *
from core.asr_backend.audio_separator import separate_audio
from core.asr_backend.audio_preprocess import process_transcription, convert_video_to_audio, split_audio, save_results, normalize_audio_volume
from core.downloader import find_video_files
from core.utils.paths import *

@check_file_exists(_2_CLEANED_CHUNKS)
def transcribe():
    # 1. æ£€æŸ¥æ˜¯å¦éœ€è¦æå–éŸ³é¢‘ï¼ˆå¦‚æœ vocal.mp3 å·²å­˜åœ¨åˆ™è·³è¿‡ï¼‰
    if not os.path.exists(_VOCAL_AUDIO_FILE):
        video_file = find_video_files()
        convert_video_to_audio(video_file)
    
    # 2. éŸ³é¢‘åˆ†ç¦»ï¼ˆäººå£°/èƒŒæ™¯ï¼‰:
    if load_key("demucs"):
        separate_audio()
        vocal_audio = normalize_audio_volume(_VOCAL_AUDIO_FILE, _VOCAL_AUDIO_FILE, format="mp3")
    else:
        # å¦‚æœæ²¡å¼€ demucs ä½† vocal å­˜åœ¨ï¼Œç›´æ¥ç”¨ vocal
        if os.path.exists(_VOCAL_AUDIO_FILE):
            vocal_audio = _VOCAL_AUDIO_FILE
        else:
            vocal_audio = _VOCAL_AUDIO_FILE

    # 3. ç”¨äººå£°æ–‡ä»¶æ£€æµ‹è¯­éŸ³è¾¹ç•Œï¼ˆé¿å…ç‰‡å¤´èƒŒæ™¯éŸ³ä¹å¹²æ‰°ï¼‰
    segments = split_audio(vocal_audio)
    
    # 4. è½¬å½•éŸ³é¢‘ç‰‡æ®µ
    all_results = []
    runtime = load_key("whisper.runtime")
    if runtime != "stable":
        raise ValueError(f"Unsupported ASR runtime: {runtime}. Only 'stable' is supported.")
    
    from core.asr_backend.stable_ts import transcribe_audio_stable as ts
    rprint("[cyan]ğŸ¤ Transcribing audio with stable-ts...[/cyan]")

    for start, end in segments:
        # åªä½¿ç”¨ vocal_audio è¿›è¡Œ ASRï¼Œä¸å†ä¼  raw
        result = ts(vocal_audio, start, end)
        all_results.append(result)
    
    # 5. åˆå¹¶ç»“æœ
    combined_result = {'segments': []}
    for result in all_results:
        combined_result['segments'].extend(result['segments'])
    
    # 6. å¤„ç†æ•°æ®
    df = process_transcription(combined_result)
    save_results(df)
        
if __name__ == "__main__":
    transcribe()