import os
import warnings
import time
import subprocess
import torch
import stable_whisper
import librosa
from rich import print as rprint
from core.utils import *

warnings.filterwarnings("ignore")
MODEL_DIR = load_key("model_dir")

@except_handler("failed to check hf mirror", default_return=None)
def check_hf_mirror():
    mirrors = {'Official': 'huggingface.co', 'Mirror': 'hf-mirror.com'}
    fastest_url = f"https://{mirrors['Official']}"
    best_time = float('inf')
    rprint("[cyan]üîç Checking HuggingFace mirrors...[/cyan]")
    for name, domain in mirrors.items():
        if os.name == 'nt':
            cmd = ['ping', '-n', '1', '-w', '3000', domain]
        else:
            cmd = ['ping', '-c', '1', '-W', '3', domain]
        start = time.time()
        result = subprocess.run(cmd, capture_output=True, text=True)
        response_time = time.time() - start
        if result.returncode == 0:
            if response_time < best_time:
                best_time = response_time
                fastest_url = f"https://{domain}"
            rprint(f"[green]‚úì {name}:[/green] {response_time:.2f}s")
    if best_time == float('inf'):
        rprint("[yellow]‚ö†Ô∏è All mirrors failed, using default[/yellow]")
    rprint(f"[cyan]üöÄ Selected mirror:[/cyan] {fastest_url} ({best_time:.2f}s)")
    return fastest_url

@except_handler("stable-ts processing error:")
def transcribe_audio_stable(vocal_audio_file, start, end):
    os.environ['HF_ENDPOINT'] = check_hf_mirror()
    WHISPER_LANGUAGE = load_key("whisper.language")
    device = "cuda" if torch.cuda.is_available() else "cpu"
    rprint(f"üöÄ Starting stable-ts using device: {device} ...")
    
    if device == "cuda":
        gpu_mem = torch.cuda.get_device_properties(0).total_memory / (1024**3)
        rprint(f"[cyan]üéÆ GPU memory:[/cyan] {gpu_mem:.2f} GB")
    
    # Âä†ËΩΩÊ®°Âûã
    if WHISPER_LANGUAGE == 'zh':
        model_name = "Belle-whisper-large-v3-zh-punct"
        local_model = os.path.join(MODEL_DIR, "Belle-whisper-large-v3-zh-punct")
    else:
        model_name = load_key("whisper.model")
        local_model = os.path.join(MODEL_DIR, model_name)
    
    if os.path.exists(local_model):
        rprint(f"[green]üì• Loading local stable-ts model:[/green] {local_model} ...")
        model_name = local_model
    else:
        rprint(f"[green]üì• Using stable-ts model from HuggingFace:[/green] {model_name} ...")
    
    model = stable_whisper.load_model(model_name, device=device, download_root=MODEL_DIR)
    
    def load_audio_segment(audio_file, start, end):
        audio, _ = librosa.load(audio_file, sr=16000, offset=start, duration=end - start, mono=True)
        return audio
    
    # Use vocal_audio_file if provided (for Demucs processed audio), otherwise it falls back to raw_audio_file in _2_asr.py
    audio_segment = load_audio_segment(vocal_audio_file, start, end)
    
    # ËΩ¨ÂΩïÂπ∂Ëé∑ÂèñËØçÁ∫ßÊó∂Èó¥Êà≥
    transcribe_start_time = time.time()
    rprint("[bold green]Note: You will see Progress if working correctly ‚Üì[/bold green]")
    
    # ÂÆâÂÖ®Â§ÑÁêÜËØ≠Ë®ÄÂèÇÊï∞
    language_arg = WHISPER_LANGUAGE
    if language_arg and 'auto' in language_arg:
        language_arg = None

    result = model.transcribe(
        audio_segment,
        language=language_arg,
        word_timestamps=True,
        verbose=False,
        regroup=False,  # Á¶ÅÁî®Ëá™Âä®ÈáçÁªÑÔºå‰øùÊåÅÊõ¥ÈïøÁöÑÂè•Â≠ê
        vad=True,       # ÂêØÁî®VADËæÖÂä©ÂÆö‰ΩçÔºå‰øÆÂ§çÂè•Â∞æÊó∂Èó¥Êà≥ÊºÇÁßª
        vad_threshold=0.35, # ÊÅ¢Â§çÈªòËÆ§ VAD ÈòàÂÄº
        min_word_dur=0.1,   # ÊÅ¢Â§çÈªòËÆ§
        suppress_silence=True,  # ÊòæÂºèÂºÄÂêØÈùôÈü≥ÊäëÂà∂
        only_voice_freq=True,   # Âè™‰øùÁïô‰∫∫Â£∞È¢ëÁéá(200-5000Hz)ÔºåËøáÊª§Â∫ïÂô™
        use_word_position=True #  ÊÅ¢Â§çÈªòËÆ§
    )
    
    #rprint("[cyan]üîß Refining timestamps...[/cyan]")
    #model.refine(audio_segment, result) # Á¶ÅÁî®ÊÖ¢ÈÄüRefine
    
    transcribe_time = time.time() - transcribe_start_time
    rprint(f"[cyan]‚è±Ô∏è time transcribe:[/cyan] {transcribe_time:.2f}s")
    
    # ËΩ¨Êç¢‰∏∫Â≠óÂÖ∏Ê†ºÂºè
    result_dict = result.to_dict()
    
    # ‰øùÂ≠òÊ£ÄÊµãÂà∞ÁöÑËØ≠Ë®Ä
    update_key("whisper.detected_language", result_dict['language'])
    # Âè™ÊúâÂΩìÁî®Êà∑ÊòéÁ°ÆÊåáÂÆö‰∫ÜÈùû‰∏≠ÊñáËØ≠Ë®ÄÔºà‰∏çÊòØ autoÔºâÊó∂ÔºåÊâçÊä•ÈîôÊèêÁ§∫
    if result_dict['language'] == 'zh' and WHISPER_LANGUAGE != 'zh' and 'auto' not in WHISPER_LANGUAGE:
        raise ValueError("Please specify the transcription language as zh and try again!")
    
    # Ë∞ÉÊï¥Êó∂Èó¥Êà≥ÔºàÂä†‰∏äËµ∑ÂßãÂÅèÁßªÔºâÂπ∂Ê∏ÖÁêÜÁ©∫Ê†º
    for segment in result_dict['segments']:
        segment['start'] += start
        segment['end'] += start
        segment['text'] = segment['text'].strip()
        
        # Á°Æ‰øùwordsÂ≠óÊÆµÂ≠òÂú®
        if 'words' not in segment:
            segment['words'] = []
        
        # Ê∏ÖÁêÜÊØè‰∏™ÂçïËØçÁöÑÂâçÂêéÁ©∫Ê†ºÂπ∂ÈáçÊñ∞ÊûÑÂª∫ÊñáÊú¨
        cleaned_words = []
        for word in segment.get('words', []):
            if 'word' in word:
                word['word'] = word['word'].strip()
                if word['word']:  # Âè™‰øùÁïôÈùûÁ©∫ÂçïËØç
                    if 'start' in word:
                        word['start'] += start
                    if 'end' in word:
                        word['end'] += start
                    cleaned_words.append(word)
        segment['words'] = cleaned_words
        
        # ÈáçÊñ∞ÊûÑÂª∫segmentÊñáÊú¨ÔºåÁ°Æ‰øùÂçïËØçÈó¥Âè™Êúâ‰∏Ä‰∏™Á©∫Ê†º
        if cleaned_words:
            segment['text'] = ' '.join([w['word'] for w in cleaned_words])
    
    # Ê∏ÖÁêÜGPUËµÑÊ∫ê
    del model
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    
    return result_dict